from requests_html import HTMLSession
from bs4 import BeautifulSoup
import pandas as pd
import time

start_time = time.time()

link = 'https://ca.jooble.org/jobs-renewable-energy-internship/Vancouver%2C-BC?p=1'

s = HTMLSession()
r = s.get(link)
soup = BeautifulSoup(r.content, 'html.parser')

# r.html.render()
data_map = {
    "child_h2_array": [],
    "url_array": [],
    "head_q": [],
    "industry": [],
    "comp_size": [],
    "website": [],
    "location": [],
    "year_fun": []
}

jobs = soup.find_all('div', attrs={'class': "heru4z"})
for job in jobs:
    child_h2 = job.find('p', attrs={'class': "z6WlhX"}).text
    data_map["child_h2_array"].append(child_h2)

modified_array = []
for word in data_map["child_h2_array"]:
    if ' ' in word:
        modified_array.append(word.replace(' ', '-'))
    else:
        modified_array.append(word)

for word in modified_array:
    data_map["url_array"].append("https://ca.jooble.org/companies/" + word)

for url in data_map["url_array"]:
    r = s.get(url)
    if r.status_code == 200:
        url_soup = BeautifulSoup(r.content, 'html.parser')
        if url_soup.find_all('li', attrs={'class': "dp_7-fPS"}):
            locates = url_soup.find_all('li', attrs={'class': "dp_7-fPS"})

            for locate in locates:
                i3 = locate.find('p')
                if i3 is not None:
                    loc = i3.get_text()
                    data_map["location"].append(loc)
                else:
                    data_map["location"].append('NaN')
            if not data_map["location"]:
                data_map["location"].append('NaN')
        if url_soup.find_all('li', attrs={'class': "dp_p6bnx"}):
            infos = url_soup.find_all('li', attrs={'class': "dp_p6bnx"})
            for info in infos:
                i1 = info.find('p')
                i2 = info.find('span')

                if info.find('h2', attrs={'class': "dp_1kv+v"}).text == 'Headquarters':
                    if i1 is not None:
                        head = i1.get_text()
                        data_map["head_q"].append(head)
                    else:
                        data_map["head_q"].append('NaN')

                elif info.find('h2', attrs={'class': "dp_1kv+v"}).text == 'Website':
                    if i2 is not None:
                        web = i2.get_text()
                        data_map["website"].append(web)
                    else:
                        data_map["website"].append('NaN')

                elif info.find('h2', attrs={'class': "dp_1kv+v"}).text == 'Industry':
                    if i1 is not None:
                        ind = i1.get_text()
                        data_map["industry"].append(ind)
                    else:
                        data_map["industry"].append('NaN')

                elif info.find('h2', attrs={'class': "dp_1kv+v"}).text == 'Company size':
                    if i1 is not None:
                        com = i1.get_text()
                        data_map["comp_size"].append(com)
                    else:
                        data_map["comp_size"].append('NaN')

                elif info.find('h2', attrs={'class': "dp_1kv+v"}).text == 'Year of foundation':
                    if i1 is not None:
                        yf = i1.get_text()
                        data_map["year_fun"].append(yf)
                    else:
                        data_map["year_fun"].append('NaN')

            if not data_map["year_fun"]:
                data_map["year_fun"].append('NaN')
            if not data_map["head_q"]:
                data_map["head_q"].append('NaN')
            if not data_map["website"]:
                data_map["website"].append('NaN')
            if not data_map["industry"]:
                data_map["industry"].append('NaN')
            if not data_map["comp_size"]:
                data_map["comp_size"].append('NaN')



    else:
        data_map["head_q"].append('NaN')
        data_map["website"].append('NaN')
        data_map["industry"].append('NaN')
        data_map["comp_size"].append('NaN')
        data_map["location"].append('NaN')
        data_map["year_fun"].append('NaN')

# Now data_map contains the data for each feature extracted from the iterations.


# Now data_rows contains the data for each row, where each row is represented as a dictionary.


# Now data_map contains the data for each feature extracted from the iterations.

print(len(data_map))
print(len(data_map["child_h2_array"]))
print(len(data_map["head_q"]))
print(len(data_map["website"]))
print(len(data_map["industry"]))
print(len(data_map["comp_size"]))
print(len(data_map["location"]))
print(len(data_map["year_fun"]))

import pandas as pd

# Your existing code for data_map

# Now create a DataFrame from the data_map
df = pd.DataFrame(data_map)

# Display the DataFrame
print(df)

import os
import pandas as pd
import time

# Your existing code for data_map

# Now create a DataFrame from the data_map
df = pd.DataFrame(data_map)

# Specify the directory where you want to save the Excel files
directory = "/Users/vedshukla/Desktop/Web_scrape"

# Generate a unique file name using timestamp
timestamp = time.strftime("%Y%m%d-%H%M%S")
file_name = f"output_{timestamp}.xlsx"

# Combine directory and file name to get the complete file path
file_path = os.path.join(directory, file_name)

# Export the DataFrame to the specified Excel file
df.to_excel(file_path, index=False)

